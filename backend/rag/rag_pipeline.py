import logging
from typing import List
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from langchain.schema import Document

logging.basicConfig(level=logging.DEBUG)  # au lieu de INFO
logger = logging.getLogger(__name__)
   
def rerank_documents(query: str, documents: List[Document], model_name, top_k: int = 5) -> List:
    """
    Rerank documents using a CrossEncoder model (like BGE-Reranker, MS-MARCO, etc.)

    Args:
        query (str): The user query.
        documents (List): A list of documents, each having `.page_content`.
        model_name: Model name (string) or already loaded model.
        top_k (int): Number of top documents to return.
    Returns:
        List: Top-k reranked documents.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)

    device = ("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()

    scores = []
    logger.info(f"üí° D√©but du reranking avec {len(documents)} documents pour la query: '{query}'")
    for doc in documents:
        inputs = tokenizer(query, doc.page_content, return_tensors="pt", padding=True, truncation=True, max_length=512).to(device)
        with torch.no_grad():
            logits = model(**inputs).logits
            score = logits[0][0].item() if logits.shape[1] == 1 else logits[0][1].item()
        scores.append(score)

    reranked = [doc for _, doc in sorted(zip(scores, documents), key=lambda x: x[0], reverse=True)]
    return reranked[:top_k]

def get_neighbors(chunk, all_chunks, k=2):
    """
    Retourne les k voisins gauche/droite d'un chunk donn√©, sans inclure le chunk lui-m√™me.
    """
    doc_source = chunk.metadata["source"]
    chunk_idx = chunk.metadata["chunk_index"]
    section_title = chunk.metadata["section_title"]
    
    # Filtrer les chunks du m√™me document et avec le m√™me titre de section
    same_doc_chunks = [c for c in all_chunks if c.metadata["source"] == doc_source and c.metadata["section_title"] == section_title]
    same_doc_chunks.sort(key=lambda x: x.metadata["chunk_index"])

    idx = next((i for i, c in enumerate(same_doc_chunks) if c.metadata["chunk_index"] == chunk_idx), None)

    if idx is None:
        return []

    neighbors = []
    
    left_neighbors = same_doc_chunks[max(0, idx - k):idx]
    neighbors.extend(left_neighbors)

    # Voisins √† droite
    right_neighbors = same_doc_chunks[idx + 1:idx + 1 + k]
    neighbors.extend(right_neighbors)

    return neighbors

def build_context(reranked_docs, all_chunks, k_neighbors=2, min_chunks=16):
    """
    Construit un contexte enrichi √† partir des top documents reranked,
    en ajoutant leurs voisins gauche/droite (via get_neighbors),
    jusqu'√† atteindre un minimum de `min_chunks` uniques.
    """
    enriched_chunks = []
    seen = set()
    total_chunks = 0

    logger.info(f"üß± D√©but de la construction du contexte avec min_chunks={min_chunks}")

    i = 0  # Index du document rerank√© actuel
    while total_chunks < min_chunks and i < len(reranked_docs):
        doc = reranked_docs[i]
        key = (doc.metadata["source"], doc.metadata["chunk_index"])

        # Ajouter le document lui-m√™me s'il n'est pas encore dans enriched_chunks
        if key not in seen:
            enriched_chunks.append(doc)
            seen.add(key)
            total_chunks += 1
            logger.info(f"üß± Ajout doc principal : {key}")

        # Ajouter ses voisins gauche/droite
        neighbors = get_neighbors(doc, all_chunks, k=k_neighbors)
        logger.info(f"üß± Voisins trouv√©s pour {key} : {len(neighbors)}")

        for neighbor in neighbors:
            n_key = (neighbor.metadata["source"], neighbor.metadata["chunk_index"])
            if n_key not in seen and \
               neighbor.metadata["source"] == doc.metadata["source"] and \
               neighbor.metadata["section_title"] == doc.metadata["section_title"]:
                enriched_chunks.append(neighbor)
                seen.add(n_key)
                total_chunks += 1
                logger.info(f"üß± Ajout voisin : {n_key}")

            if total_chunks >= min_chunks:
                break

        i += 1

    if total_chunks < min_chunks:
        logger.warning(f"üõë Nombre insuffisant de chunks apr√®s enrichissement : {total_chunks}/{min_chunks}")

    # Tri logique par source et index
    enriched_chunks.sort(key=lambda x: (x.metadata["source"], x.metadata["chunk_index"]))

    # Construction du texte de contexte avec s√©parations par source
    previous_source = None
    separator = "\n\n"
    contexte_parts = []

    for chunk in enriched_chunks:
        current_source = chunk.metadata["source"]
        if current_source != previous_source and previous_source is not None:
            contexte_parts.append("\n------- Nouvelle source :\n")
        contexte_parts.append(chunk.page_content)
        previous_source = current_source

    contexte = separator.join(contexte_parts)

    logger.info(f"üß± Contexte construit avec {total_chunks} chunks.")
    logger.info(f"Contexte final (preview 10000c): {contexte[:10000]}")

    return contexte, enriched_chunks

def find_most_relevant_sections(question: str, sections: List[str], model_name: str, top_k: int = 3) -> List[str]:
    """
    Trouve les sections les plus pertinentes en fonction de la similarit√© entre la question et chaque titre de section.

    Args:
        question (str): La question pos√©e par l'utilisateur.
        sections (List[str]): Liste de titres de section (strings).
        model_name (str): Le nom du mod√®le utilis√© pour calculer la similarit√©.
        top_k (int): Le nombre de sections les plus pertinentes √† renvoyer.

    Returns:
        List[str]: Liste des top-k titres de section les plus pertinents.
    """

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)
    model.eval()

    scores = []

    print(f"üîç Calcul de la similarit√© pour la question : {question}")

    for section_title in sections:
        print(f"Comparaison avec le titre : '{section_title}'")

        inputs = tokenizer(question, section_title, return_tensors="pt", padding=True, truncation=True, max_length=512).to(device)

        with torch.no_grad():
            logits = model(**inputs).logits
            score = logits[0][1].item() if logits.shape[1] == 2 else logits[0][0].item()

        print(f"Score : {score}")
        scores.append((score, section_title))

    reranked_sections = [title for score, title in sorted(scores, key=lambda x: x[0], reverse=True)[:top_k]]

    print(f"üîù Sections les plus pertinentes : {reranked_sections}")
    return reranked_sections

def should_use_section_filter(question: str, sections: List[str], model_name: str, top_diff: float = 1.0) -> bool:
    """
    D√©cide si le filtrage par section est pertinent en comparant le meilleur score au deuxi√®me.
    Cela √©vite de d√©pendre d'un seuil absolu de similarit√© (non fiable sur des logits).
    
    Args:
        question (str): Question utilisateur.
        sections (List[str]): Titres de sections.
        model_name (str): Nom du mod√®le utilis√©.
        top_diff (float): Diff√©rence minimale entre le meilleur et le deuxi√®me score pour d√©clencher le filtre.

    Returns:
        bool: True si filtrage pertinent, sinon False.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)
    model.eval()

    scores = []
    for section_title in sections:
        inputs = tokenizer(question, section_title, return_tensors="pt", padding=True, truncation=True, max_length=512).to(device)
        with torch.no_grad():
            logits = model(**inputs).logits
            score = logits[0][1].item() if logits.shape[1] == 2 else logits[0][0].item()
            scores.append(score)

    sorted_scores = sorted(scores, reverse=True)
    logger.info(f"üìä Top 2 scores section: {sorted_scores[:2]}")
    
    if len(sorted_scores) < 2:
        return True  # Si une seule section, on filtre forc√©ment
    use_filter = (sorted_scores[0] - sorted_scores[1]) >= top_diff

    if use_filter:
        logger.info(f"‚úÖ Filtrage activ√© : √©cart de score ({sorted_scores[0] - sorted_scores[1]:.2f}) ‚â• seuil ({top_diff})")
    else:
        logger.info(f"‚ùå Filtrage d√©sactiv√© : √©cart de score ({sorted_scores[0] - sorted_scores[1]:.2f}) < seuil ({top_diff})")

    return use_filter

def get_title_documents(documents: List[Document], source) -> List[str]:
    """
    Extrait une liste de titres de sections uniques √† partir des documents dont le type est 'title'.
    """
    titres_uniques = set()

    for doc in documents:
        if doc.metadata["source"] == source:
            titres_uniques.add(doc.metadata["section_title"])

    return sorted(titres_uniques)

def poser_question(question, vectorstore, llm, workspace_id, allowed_levels, reranker_model, k=100, top_n_reranked=5, k_neighbors=3, top_k=1) -> dict:
    logger.info(f"‚ùì Question pos√©e : {question}")
    logger.info(f"üîë Workspace ID demand√© : {workspace_id}")
    logger.info(f"üîí Niveaux de confidentialit√© autoris√©s : {allowed_levels}")

    try:
        # üî¢ Nombre total de documents dans la classe 'AO' (toute la base)
        weaviate_client = vectorstore._client
        count_result = weaviate_client.query.aggregate("AO").with_meta_count().do()
        count = count_result['data']['Aggregate']['AO'][0]['meta']['count']
        logger.info(f"üî¢ Nombre de documents dans l'index 'AO' : {count}")
    
        # Cr√©e un retriever avec filtre sur workspace et niveaux de confidentialit√©
        retriever = vectorstore.as_retriever(search_kwargs={
            "k": count  # ou un chiffre plus raisonnable si tu veux pas reranker 300 docs √† chaque fois
        })
        # Recherche vectorielle filtr√©e
        docs = retriever.get_relevant_documents(query = question)
        logger.info(f"üìÑ Documents trouv√©s avant filtrage : {len(docs)}")

        # √âtape 2 : filtre √† la main sur le workspace_id
        filtered_docs = [
            doc for doc in docs if doc.metadata["workspace_id"] == workspace_id and doc.metadata["confidentiality"] in allowed_levels
        ]
        
        # Logs pour le d√©bogage du filtrage
        for doc in docs[:5]:  # On affiche les 5 premiers documents pour v√©rifier
            logger.info(f"üìù Document avant filtrage - Workspace: {doc.metadata['workspace_id']}, Confidentiality: {doc.metadata['confidentiality']}")
        
        docs = filtered_docs[:k]
        logger.info(f"üîç {len(docs)} document(s) trouv√©(s) apr√®s filtrage vectoriel.")
        
        if not docs:
            logger.warning("‚ùå Aucun document trouv√© apr√®s filtrage. V√©rifiez que :")
            logger.warning(f"   - Le workspace_id '{workspace_id}' existe dans les documents")
            logger.warning(f"   - Les niveaux de confidentialit√© {allowed_levels} correspondent aux documents")
            return {"response": "Aucun document pertinent trouv√©.", "sources": []}

        # Logs pour les documents filtr√©s
        for doc in docs[:5]:  # On affiche les 5 premiers documents filtr√©s
            logger.info(f"‚úÖ Document apr√®s filtrage - Workspace: {doc.metadata.get('workspace_id')}, Confidentiality: {doc.metadata.get('confidentiality')}")

        source = docs[0].metadata["source"]
        sections = get_title_documents(docs, source)
        logger.info(f"üìö Sections trouv√©es : {sections}")

        if should_use_section_filter(question, sections, reranker_model):
            reranked_sections = find_most_relevant_sections(question, sections, reranker_model, top_k)
            docs = [
                doc for doc in docs if doc.metadata["section_title"] in reranked_sections
            ]
            logger.info(f"üîç {len(docs)} document(s) trouv√©(s) apr√®s filtrage sections.")
        else:
            logger.info(f"üì≠ Filtrage par section ignor√© (aucune section n'est suffisamment pertinente).")

        if not docs:
            logger.warning("Aucun document trouv√© apr√®s filtrage.")
            return {"response": "Aucun document pertinent trouv√©.", "sources": []}

        # Reranking
        reranked_docs = rerank_documents(question, docs, reranker_model, top_n_reranked)
        logger.info(f"üîç {len(reranked_docs[:top_n_reranked])} document(s) trouv√©(s) apr√®s reranking.")

        if not reranked_docs:
            logger.warning("Aucun document pertinent trouv√© apr√®s reranking.")
            return {"response": "Aucun document pertinent trouv√©.", "sources": []}

        logger.info(f"üîù Reranked docs: {[ (doc.metadata['source'], doc.metadata['chunk_index']) for doc in reranked_docs ]}")

        # Enrichissement de contexte (optionnel)
        contexte, enriched_chunks = build_context(reranked_docs, all_chunks=filtered_docs, k_neighbors=k_neighbors)
        logger.info(f"üîç {len(enriched_chunks)} document(s) trouv√©(s) apr√®s enrichissement.")

        # G√©n√©ration finale
        prompt = (
            f"Tu es un assistant intelligent. Tu dois r√©pondre en fran√ßais √† la question suivante en t'appuyant **exclusivement** sur le texte fourni. "
            f"Ne m√©lange pas les sources fournies et ne compl√®te jamais avec des connaissances ext√©rieures.\n\n"
            f"### Contexte :\n{contexte}\n"
            f"### Question :\n{question}\n"
            f"### R√©ponse :"
        )
        response = llm.invoke(prompt)

        sources = [
            {
                "title": doc.metadata["source"],
                "content": doc.page_content,
                "page": doc.metadata.get("page_num")  # Optionnel, peut √™tre None
            }
            for doc in enriched_chunks
        ]

        logger.info(f"‚úÖ R√©ponse g√©n√©r√©e : {response[:100]}...")
        return {"response": response, "sources": sources}

    except Exception as e:
        logger.error(f"Erreur lors du traitement de la question : {e}")
        return {"response": "Une erreur s'est produite lors du traitement de la question.", "sources": []}